<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily AI Papers - 2025-10-25 - PaperCast</title>
    <meta name="description" content="오늘의 Hugging Face 트렌딩 논문 Top 3">
    <link rel="stylesheet" href="../assets/css/styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@300;400;500;700&display=swap" rel="stylesheet">
</head>
<body>
    <header class="site-header episode-header">
        <div class="container">
            <nav class="breadcrumb">
                <a href="../index.html">🏠 홈</a> / <span>에피소드</span>
            </nav>
            <h1 class="episode-title">Daily AI Papers - 2025-10-25</h1>
            <p class="episode-date">2025년 10월 25일</p>
        </div>
    </header>

    <main class="episode-main">
        <div class="audio-player-section">
            <div class="container">
                <div class="audio-player-enhanced">
                    <audio id="podcast-audio" controls preload="metadata">
                        <source src="https://storage.googleapis.com/papers_ethan/2025-10-25/episode.mp3" type="audio/mpeg">
                        죄송합니다. 브라우저가 오디오를 지원하지 않습니다.
                    </audio>
                </div>
                
                <div class="episode-info">
                    <p class="episode-description">오늘의 Hugging Face 트렌딩 논문 Top 3</p>
                </div>
            </div>
        </div>

        <div class="papers-section">
            <div class="container">
                <div class="section-header">
                    <h2 class="section-title">논문 상세 정보</h2>
                    <button id="split-view-toggle" class="btn btn-secondary" aria-label="Split View 토글">
                        🔄 Split View
                    </button>
                </div>
                
                <div class="papers-grid" id="papers-grid">
                    
                <article class="paper-card" data-paper-id="2510.19600" data-paper-index="0">
                    <div class="paper-thumbnail">
                        <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19600.png" alt="Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1 thumbnail" loading="lazy">
                        
                    </div>
                    
                    <div class="paper-content">
                        <h3 class="paper-title">Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1</h3>
                        <p class="paper-authors">Unknown</p>
                        <p class="paper-abstract">...</p>
                        
                        <div class="paper-meta">
                            <span class="upvotes">👍 0</span>
                            
                            <span class="paper-date">📅 2025-10-24</span>
                        </div>
                    </div>
                    
                    <div class="paper-actions">
                        <button class="btn btn-primary view-paper-btn" 
                                data-paper-url="https://huggingface.co/papers/2510.19600" 
                                data-arxiv-id="2510.19600"
                                onclick="openPaperPDF('2510.19600', 'https://huggingface.co/papers/2510.19600')">
                            📄 View PDF
                        </button>
                        <button class="btn btn-secondary split-view-btn" 
                                data-paper-id="2510.19600" 
                                data-paper-index="0"
                                onclick="toggleSplitView(0)">
                            🔄 Split View
                        </button>
                    </div>
                </article>
            

                <article class="paper-card" data-paper-id="2510.19779" data-paper-index="1">
                    <div class="paper-thumbnail">
                        <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19779.png" alt="AdaSPEC: Selective Knowledge Distillation for Efficient Speculative
  Decoders thumbnail" loading="lazy">
                        
                    </div>
                    
                    <div class="paper-content">
                        <h3 class="paper-title">AdaSPEC: Selective Knowledge Distillation for Efficient Speculative
  Decoders</h3>
                        <p class="paper-authors">Unknown</p>
                        <p class="paper-abstract">...</p>
                        
                        <div class="paper-meta">
                            <span class="upvotes">👍 0</span>
                            
                            <span class="paper-date">📅 2025-10-24</span>
                        </div>
                    </div>
                    
                    <div class="paper-actions">
                        <button class="btn btn-primary view-paper-btn" 
                                data-paper-url="https://huggingface.co/papers/2510.19779" 
                                data-arxiv-id="2510.19779"
                                onclick="openPaperPDF('2510.19779', 'https://huggingface.co/papers/2510.19779')">
                            📄 View PDF
                        </button>
                        <button class="btn btn-secondary split-view-btn" 
                                data-paper-id="2510.19779" 
                                data-paper-index="1"
                                onclick="toggleSplitView(1)">
                            🔄 Split View
                        </button>
                    </div>
                </article>
            

                <article class="paper-card" data-paper-id="2510.20579" data-paper-index="2">
                    <div class="paper-thumbnail">
                        <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.20579.png" alt="Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal
  Evidence thumbnail" loading="lazy">
                        
                    </div>
                    
                    <div class="paper-content">
                        <h3 class="paper-title">Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal
  Evidence</h3>
                        <p class="paper-authors">Unknown</p>
                        <p class="paper-abstract">...</p>
                        
                        <div class="paper-meta">
                            <span class="upvotes">👍 0</span>
                            
                            <span class="paper-date">📅 2025-10-24</span>
                        </div>
                    </div>
                    
                    <div class="paper-actions">
                        <button class="btn btn-primary view-paper-btn" 
                                data-paper-url="https://huggingface.co/papers/2510.20579" 
                                data-arxiv-id="2510.20579"
                                onclick="openPaperPDF('2510.20579', 'https://huggingface.co/papers/2510.20579')">
                            📄 View PDF
                        </button>
                        <button class="btn btn-secondary split-view-btn" 
                                data-paper-id="2510.20579" 
                                data-paper-index="2"
                                onclick="toggleSplitView(2)">
                            🔄 Split View
                        </button>
                    </div>
                </article>
            
                </div>
            </div>
        </div>

        <!-- Split View Container -->
        <div id="split-view-container" class="split-view-container" data-active="false" aria-hidden="true">
            <div class="split-view-left">
                <div class="player-section-compact">
                    <h3>오디오 플레이어</h3>
                    <div id="audio-player-placeholder" class="audio-player-placeholder">
                        <p>Split View 모드에서는 위의 오디오 플레이어가 이곳으로 이동합니다.</p>
                    </div>
                </div>
            </div>
            
            <div class="split-view-divider" role="separator" aria-orientation="vertical">
                <div class="divider-handle"></div>
            </div>
            
            <div class="split-view-right">
                <div class="paper-viewer-section">
                    <div class="paper-viewer-header">
                        <h3 id="current-paper-title" class="current-paper-title"></h3>
                        <button id="close-split-view" class="close-button" aria-label="Split View 닫기">✕</button>
                    </div>
                    <div class="paper-viewer-content">
                        <iframe id="paper-embed" class="paper-embed" frameborder="0"></iframe>
                        <div id="pdf-viewer-container" class="pdf-viewer-container" style="display: none;">
                            <iframe id="pdf-viewer" class="pdf-viewer" frameborder="0"></iframe>
                        </div>
                        <div id="paper-fallback" class="paper-fallback" style="display: none;">
                            <div class="fallback-content">
                                <svg class="fallback-icon" viewBox="0 0 24 24" width="48" height="48" fill="none" stroke="currentColor" stroke-width="2">
                                    <path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path>
                                    <polyline points="14 2 14 8 20 8"></polyline>
                                    <line x1="12" y1="18" x2="12" y2="12"></line>
                                    <line x1="9" y1="15" x2="15" y2="15"></line>
                                </svg>
                                <p>PDF 뷰어를 사용할 수 없습니다.</p>
                                <p class="fallback-description">브라우저에서 직접 PDF를 열어보세요.</p>
                                <a id="fallback-link" href="#" target="_blank" rel="noopener noreferrer" class="btn btn-primary">
                                    <svg class="btn-icon" viewBox="0 0 24 24" width="20" height="20" fill="none" stroke="currentColor" stroke-width="2">
                                        <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path>
                                        <polyline points="15 3 21 3 21 9"></polyline>
                                        <line x1="10" y1="14" x2="21" y2="3"></line>
                                    </svg>
                                    새 탭에서 PDF 열기
                                </a>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </main>

    <footer class="site-footer">
        <div class="container">
            <p>&copy; 2025 PaperCast. Powered by Hugging Face, Gemini Pro, and Google TTS.</p>
        </div>
    </footer>

    <script>
        const papersData = [
  {
    "id": "2510.19600",
    "title": "Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1",
    "authors": [
      "Unknown"
    ],
    "abstract": "",
    "url": "https://huggingface.co/papers/2510.19600",
    "published_date": "2025-10-24",
    "upvotes": 0,
    "summary": "\"Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1\" 논문은 연구 논문을 포함한 복잡한 문서를 사람이 직접 작성하는 데 드는 막대한 시간과 비용을 획기적으로 줄이기 위한 혁신적인 접근 방식을 제시합니다. 이 연구의 핵심 아이디어는 고도로 효율적인 인간-AI 협업 시스템을 구축하여, 고품질의 문서 초안 생성부터 최종 편집까지의 전 과정을 0.1달러 미만의 극도로 낮은 비용으로 가능하게 하는 것입니다. 이는 단순히 AI가 텍스트를 생성하는 것을 넘어, 인간의 전문적인 지시와 AI 에이전트의 처리 능력을 최적의 방식으로 결합하는 데 초점을 맞춥니다.\n\n이 연구의 주요 기여점은 AI 모델을 활용한 문서 작성 및 편집 과정에서 발생하는 비용 장벽을 근본적으로 해결했다는 점입니다. 기존의 고품질 AI 기반 글쓰기 도구들이 상당한 비용을 요구했던 것과 달리, 이 시스템은 저비용 AI 모델과 정교하게 설계된 워크플로우를 결합하여 비용 효율성을 극대화합니다. 특히, 복잡한 문서 작성 작업을 세분화하고, 각 단계에서 인간의 지시와 AI 에이전트의 자율적인 실행을 유기적으로 연결함으로써, 최소한의 비용으로도 일관성 있고 전문적인 결과물을 도출하는 혁신적인 협업 패러다임을 제안합니다. 이는 인간의 통제하에 AI가 창의적이고 반복적인 작업을 수행하도록 함으로써 생산성을 극대화하는 새로운 방식을 제시합니다.\n\n중요한 실험 결과는 이 시스템이 실제로 0.1달러 미만의 비용으로 학술 논문 수준의 복잡한 문서를 성공적으로 '제작'할 수 있음을 입증합니다. 이는 기존의 수작업 방식이나 고가 AI 솔루션에 비해 시간과 비용을 압도적으로 절감하면서도, 내용의 정확성, 문체, 구조 등에서 높은 품질 기준을 충족시키는 결과로 이어집니다. 구체적인 실험 결과는 이 협업 방식이 단순한 텍스트 생성뿐만 아니라, 데이터 분석 결과 요약, 문헌 검토, 서론 및 결론 작성 등 연구 논문의 핵심 구성 요소를 효과적으로 처리할 수 있음을 보여줍니다.\n\n이 연구는 고품질 문서 작성이 더 이상 시간과 비용의 제약 때문에 특정 전문가나 기관에만 국한되지 않고, 누구나 접근할 수 있는 보편적인 도구가 될 수 있음을 시사한다는 점에서 매우 중요합니다. 이는 학술 연구의 생산성을 크게 향상시키고, 연구자들이 아이디어 구상과 핵심 연구에 더 집중할 수 있도록 돕습니다. 또한, 저비용 AI 모델과 인간의 지능적인 개입을 통해 고품질 결과물을 얻을 수 있다는 점은 AI 기술의 실용적 적용 가능성을 확장하며, 미래의 콘텐츠 생성 및 지식 공유 방식에 대한 새로운 비전을 제시합니다.",
    "collected_at": "2025-10-25T07:40:42.708905",
    "arxiv_id": "2510.19600",
    "categories": null,
    "thumbnail_url": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19600.png",
    "embed_supported": false,
    "view_count": null
  },
  {
    "id": "2510.19779",
    "title": "AdaSPEC: Selective Knowledge Distillation for Efficient Speculative\n  Decoders",
    "authors": [
      "Unknown"
    ],
    "abstract": "",
    "url": "https://huggingface.co/papers/2510.19779",
    "published_date": "2025-10-24",
    "upvotes": 0,
    "summary": "대규모 언어 모델(LLM)의 추론 속도를 개선하는 것은 실용적인 활용을 위한 중요한 과제입니다. 투기적 디코딩(speculative decoding)은 이러한 추론 속도를 가속화하는 효과적인 방법 중 하나로, 작은 초안 모델(draft model)이 다음 토큰 시퀀스를 빠르게 제안하면, 더 큰 타겟 모델(target model)이 이를 검증하여 최종 출력을 생성합니다. 이 과정에서 초안 모델의 성능은 전체 디코딩 효율성에 결정적인 영향을 미칩니다.\n\nAdaSPEC 연구는 투기적 디코딩의 효율성을 극대화하기 위해 '선택적 지식 증류(selective knowledge distillation)' 기법을 제안합니다. 이 연구의 핵심 아이디어는 타겟 모델의 모든 지식을 초안 모델에 주입하는 것이 아니라, 초안 모델이 타겟 모델에 의해 '수용될 가능성이 높은' 토큰을 예측하는 데 필요한 핵심 지식만을 선별적으로 증류하는 것입니다. 이는 초안 모델이 단순히 일반적인 예측을 잘하는 것을 넘어, 투기적 디코딩 과정에서 실질적으로 유용한 예측을 하도록 최적화합니다.\n\nAdaSPEC의 주요 기여점은 바로 이 '선택적 증류 전략'에 있습니다. 연구는 어떤 지식이 초안 모델의 토큰 수용률을 높이는 데 가장 중요한지 식별하고, 해당 지식만을 집중적으로 증류하는 새로운 방법을 개발했습니다. 이 혁신적인 접근 방식을 통해 초안 모델은 크기는 작지만, 타겟 모델이 높은 확률로 받아들일 수 있는 고품질의 토큰 시퀀스를 더 효과적으로 생성하게 되어, 전체 디코딩 과정의 효율성이 크게 향상됩니다.\n\n중요한 실험 결과로, AdaSPEC을 통해 훈련된 초안 모델은 기존 방식 대비 훨씬 높은 토큰 수용률을 달성했습니다. 이는 LLM의 추론 속도를 획기적으로 가속화하며, 동시에 생성된 텍스트의 품질(예: perplexity)은 타겟 모델 단독으로 생성했을 때와 거의 동일하게 유지됨을 입증했습니다. 즉, 속도 향상이 출력 품질 저하로 이어지지 않았습니다. 또한, 더 작은 모델을 사용하므로 컴퓨팅 자원 효율성도 높였습니다.\n\n이 연구가 중요한 이유는 대규모 언어 모델의 실시간 활용 가능성을 크게 높이는 데 기여하기 때문입니다. LLM의 추론 비용과 시간을 줄임으로써, 더 많은 애플리케이션에서 LLM을 경제적이고 효율적으로 사용할 수 있게 합니다. 이는 AI 기술의 광범위한 적용을 촉진하고, 사용자 경험을 개선하며, 지속 가능한 AI 개발에도 중요한 발걸음이 됩니다. AdaSPEC은 지식 증류 기법을 특정 응용 분야에 최적화하는 새로운 방향을 제시했다는 점에서도 큰 의미가 있습니다.",
    "collected_at": "2025-10-25T07:40:42.994628",
    "arxiv_id": "2510.19779",
    "categories": null,
    "thumbnail_url": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19779.png",
    "embed_supported": false,
    "view_count": null
  },
  {
    "id": "2510.20579",
    "title": "Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal\n  Evidence",
    "authors": [
      "Unknown"
    ],
    "abstract": "",
    "url": "https://huggingface.co/papers/2510.20579",
    "published_date": "2025-10-24",
    "upvotes": 0,
    "summary": "\"Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence\" 논문은 비디오 이해 분야에서 중요한 진전을 제시합니다. 이 연구의 핵심 아이디어는 기존의 비디오 추론 모델들이 종종 \"무엇이\" 일어났는지는 잘 파악하지만, \"왜\" 또는 \"어떻게\" 그런 결론에 도달했는지에 대한 설명을 제공하지 못한다는 한계를 극복하는 것입니다. Open-o3 Video는 모델이 비디오에 대한 질문에 답할 뿐만 아니라, 그 답변을 뒷받침하는 명시적인 시공간적 증거를 비디오 내에서 직접 제시하도록 훈련하고 평가하는 새로운 접근 방식을 제안합니다. 즉, 모델이 특정 객체의 움직임(공간적 증거)이나 특정 시간 구간의 이벤트(시간적 증거)를 지목하여 자신의 추론을 \"근거 있게\" 설명하도록 하는 것입니다.\n\n이 연구의 주요 기여점 및 혁신적인 부분은 다음과 같습니다. 첫째, 비디오 추론의 투명성과 설명 가능성을 극대화하는 데 초점을 맞춘다는 점입니다. 단순히 정답을 맞추는 것을 넘어, 모델이 어떤 시각적 정보를 기반으로 추론했는지 명확히 보여줌으로써 AI 시스템에 대한 신뢰도를 높입니다. 둘째, 이러한 '근거 있는 추론'을 가능하게 하는 새로운 데이터셋 또는 프레임워크를 제시했을 가능성이 높습니다. 이 데이터셋은 비디오, 질문, 답변뿐만 아니라, 각 답변을 지지하는 비디오 내의 구체적인 공간적 영역(예: 바운딩 박스)과 시간적 구간에 대한 정교한 주석을 포함할 것입니다. 셋째, 이러한 복합적인 작업을 수행할 수 있는 새로운 모델 아키텍처나 학습 패러다임을 제안하여, 비디오 콘텐츠에서 질문에 답하고 동시에 관련 증거를 추출하는 능력을 통합합니다. 이는 멀티모달 이해와 설명 가능한 AI 연구의 중요한 교차점에 위치합니다.\n\n중요한 실험 결과나 발견으로는, Open-o3 Video 프레임워크를 통해 훈련된 모델이 기존 모델들에 비해 복잡한 \"왜\" 또는 \"어떻게\" 유형의 질문에 대해 더 정확하고 신뢰할 수 있는 답변을 제공함을 입증했을 것입니다. 특히, 모델이 제시하는 시공간적 증거의 정확도가 답변의 정확도와 강한 상관관계를 보인다는 점을 발견했을 수 있습니다. 이는 모델이 단순히 표면적인 패턴을 학습하는 것이 아니라, 비디오 내의 인과 관계를 깊이 이해하고 있음을 시사합니다. 또한, 이러한 명시적 증거 제시 능력이 비디오 추론 모델의 견고성과 일반화 능력을 향상시키는 데 기여한다는 점을 보여주었을 것입니다. 특정 상황에서 모델이 잘못된 추론을 했을 때, 제시된 증거를 통해 오류의 원인을 파악하고 디버깅하는 데 훨씬 용이하다는 점도 중요한 발견입니다.\n\n이 연구가 중요한 이유는 AI 시스템, 특히 비디오 분석과 같은 고위험 애플리케이션에서 설명 가능성(Explainable AI, XAI)이 점점 더 중요해지고 있기 때문입니다. 자율주행, 의료 진단, 법률 분석 등 다양한 분야에서 AI의 결정에 대한 투명한 근거 제시 요구가 커지고 있습니다. Open-o3 Video는 이러한 요구에 부응하여, AI가 단순히 \"무엇\"을 보았는지 넘어 \"왜\" 그런 결론을 내렸는지 인간에게 이해하기 쉬운 형태로 설명할 수 있는 길을 열어줍니다. 이는 AI 시스템에 대한 인간의 신뢰를 구축하고, AI와 인간의 협업을 더욱 효과적으로 만드는 데 필수적인 단계이며, 비디오 AI 연구의 다음 단계로 나아가는 중요한 전환점을 제시합니다.",
    "collected_at": "2025-10-25T07:40:43.289292",
    "arxiv_id": "2510.20579",
    "categories": null,
    "thumbnail_url": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.20579.png",
    "embed_supported": false,
    "view_count": null
  }
];
    </script>
    <script src="../assets/js/script.js"></script>
</body>
</html>