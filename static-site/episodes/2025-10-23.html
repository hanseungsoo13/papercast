<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily AI Papers - 2025-10-23 - PaperCast</title>
    <meta name="description" content="오늘의 Hugging Face 트렌딩 논문 Top 3">
    <link rel="stylesheet" href="../assets/css/styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@300;400;500;700&display=swap" rel="stylesheet">
</head>
<body>
    <header class="site-header episode-header">
        <div class="container">
            <nav class="breadcrumb">
                <a href="../index.html">🏠 홈</a> / <span>에피소드</span>
            </nav>
            <h1 class="episode-title">Daily AI Papers - 2025-10-23</h1>
            <p class="episode-date">2025년 10월 22일</p>
        </div>
    </header>

    <main class="episode-main">
        <div class="audio-player-section">
            <div class="container">
                <div class="audio-player-enhanced">
                    <audio id="podcast-audio" controls preload="metadata">
                        <source src="https://storage.googleapis.com/papers_ethan/2025-10-23/episode.mp3" type="audio/mpeg">
                        죄송합니다. 브라우저가 오디오를 지원하지 않습니다.
                    </audio>
                </div>
                
                <div class="episode-info">
                    <p class="episode-description">오늘의 Hugging Face 트렌딩 논문 Top 3</p>
                </div>
            </div>
        </div>

        <div class="papers-section">
            <div class="container">
                <div class="section-header">
                    <h2 class="section-title">논문 상세 정보</h2>
                    <button id="split-view-toggle" class="btn btn-secondary" aria-label="Split View 토글">
                        🔄 Split View
                    </button>
                </div>
                
                <div class="papers-grid" id="papers-grid">
                    
                <article class="paper-card" data-paper-id="2510.07581" data-paper-index="0">
                    <div class="paper-thumbnail">
                        <img src="../assets/images/placeholder.png" alt="Expanding the Action Space of LLMs to Reason Beyond Language thumbnail" loading="lazy">
                        
                    </div>
                    
                    <div class="paper-content">
                        <h3 class="paper-title">Expanding the Action Space of LLMs to Reason Beyond Language</h3>
                        <p class="paper-authors">Zhongqi Yue, Weishi Wang, Yundaichuan Zhan 외 3명</p>
                        <p class="paper-abstract">Large Language Models (LLMs) are powerful reasoners in natural language, but
their actions are typically confined to outputting vocabulary tokens. As a
result, interactions with external environments ...</p>
                        
                        <div class="paper-meta">
                            <span class="upvotes">👍 1</span>
                            
                            <span class="paper-date">📅 2025-10-08</span>
                        </div>
                    </div>
                    
                    <div class="paper-actions">
                        <button class="btn btn-primary view-paper-btn" 
                                data-paper-url="https://huggingface.co/papers/2510.07581" 
                                data-arxiv-id=""
                                onclick="openPaperPDF('', 'https://huggingface.co/papers/2510.07581')">
                            📄 View PDF
                        </button>
                        <button class="btn btn-secondary split-view-btn" 
                                data-paper-id="2510.07581" 
                                data-paper-index="0"
                                onclick="toggleSplitView(0)">
                            🔄 Split View
                        </button>
                    </div>
                </article>
            

                <article class="paper-card" data-paper-id="2510.15710" data-paper-index="1">
                    <div class="paper-thumbnail">
                        <img src="../assets/images/placeholder.png" alt="Unimedvl: Unifying Medical Multimodal Understanding And Generation
  Through Observation-Knowledge-Analysis thumbnail" loading="lazy">
                        
                    </div>
                    
                    <div class="paper-content">
                        <h3 class="paper-title">Unimedvl: Unifying Medical Multimodal Understanding And Generation
  Through Observation-Knowledge-Analysis</h3>
                        <p class="paper-authors">Junzhi Ning, Wei Li, Cheng Tang 외 24명</p>
                        <p class="paper-abstract">Medical diagnostic applications require models that can process multimodal
medical inputs (images, patient histories, lab results) and generate diverse
outputs including both textual reports and visua...</p>
                        
                        <div class="paper-meta">
                            <span class="upvotes">👍 1</span>
                            
                            <span class="paper-date">📅 2025-10-17</span>
                        </div>
                    </div>
                    
                    <div class="paper-actions">
                        <button class="btn btn-primary view-paper-btn" 
                                data-paper-url="https://huggingface.co/papers/2510.15710" 
                                data-arxiv-id=""
                                onclick="openPaperPDF('', 'https://huggingface.co/papers/2510.15710')">
                            📄 View PDF
                        </button>
                        <button class="btn btn-secondary split-view-btn" 
                                data-paper-id="2510.15710" 
                                data-paper-index="1"
                                onclick="toggleSplitView(1)">
                            🔄 Split View
                        </button>
                    </div>
                </article>
            

                <article class="paper-card" data-paper-id="2510.18632" data-paper-index="2">
                    <div class="paper-thumbnail">
                        <img src="../assets/images/placeholder.png" alt="Think with 3D: Geometric Imagination Grounded Spatial Reasoning from
  Limited Views thumbnail" loading="lazy">
                        
                    </div>
                    
                    <div class="paper-content">
                        <h3 class="paper-title">Think with 3D: Geometric Imagination Grounded Spatial Reasoning from
  Limited Views</h3>
                        <p class="paper-authors">Zhangquan Chen, Manyuan Zhang, Xinlei Yu 외 7명</p>
                        <p class="paper-abstract">Though recent advances in vision-language models (VLMs) have achieved
remarkable progress across a wide range of multimodal tasks, understanding 3D
spatial relationships from limited views remains a s...</p>
                        
                        <div class="paper-meta">
                            <span class="upvotes">👍 1</span>
                            
                            <span class="paper-date">📅 2025-10-21</span>
                        </div>
                    </div>
                    
                    <div class="paper-actions">
                        <button class="btn btn-primary view-paper-btn" 
                                data-paper-url="https://huggingface.co/papers/2510.18632" 
                                data-arxiv-id=""
                                onclick="openPaperPDF('', 'https://huggingface.co/papers/2510.18632')">
                            📄 View PDF
                        </button>
                        <button class="btn btn-secondary split-view-btn" 
                                data-paper-id="2510.18632" 
                                data-paper-index="2"
                                onclick="toggleSplitView(2)">
                            🔄 Split View
                        </button>
                    </div>
                </article>
            
                </div>
            </div>
        </div>

        <!-- Split View Container -->
        <div id="split-view-container" class="split-view-container" data-active="false" aria-hidden="true">
            <div class="split-view-left">
                <div class="player-section-compact">
                    <h3>오디오 플레이어</h3>
                    <div id="audio-player-placeholder" class="audio-player-placeholder">
                        <p>Split View 모드에서는 위의 오디오 플레이어가 이곳으로 이동합니다.</p>
                    </div>
                </div>
            </div>
            
            <div class="split-view-divider" role="separator" aria-orientation="vertical">
                <div class="divider-handle"></div>
            </div>
            
            <div class="split-view-right">
                <div class="paper-viewer-section">
                    <div class="paper-viewer-header">
                        <h3 id="current-paper-title" class="current-paper-title"></h3>
                        <button id="close-split-view" class="close-button" aria-label="Split View 닫기">✕</button>
                    </div>
                    <div class="paper-viewer-content">
                        <iframe id="paper-embed" class="paper-embed" frameborder="0"></iframe>
                        <div id="pdf-viewer-container" class="pdf-viewer-container" style="display: none;">
                            <iframe id="pdf-viewer" class="pdf-viewer" frameborder="0"></iframe>
                        </div>
                        <div id="paper-fallback" class="paper-fallback" style="display: none;">
                            <div class="fallback-content">
                                <svg class="fallback-icon" viewBox="0 0 24 24" width="48" height="48" fill="none" stroke="currentColor" stroke-width="2">
                                    <path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path>
                                    <polyline points="14 2 14 8 20 8"></polyline>
                                    <line x1="12" y1="18" x2="12" y2="12"></line>
                                    <line x1="9" y1="15" x2="15" y2="15"></line>
                                </svg>
                                <p>PDF 뷰어를 사용할 수 없습니다.</p>
                                <p class="fallback-description">브라우저에서 직접 PDF를 열어보세요.</p>
                                <a id="fallback-link" href="#" target="_blank" rel="noopener noreferrer" class="btn btn-primary">
                                    <svg class="btn-icon" viewBox="0 0 24 24" width="20" height="20" fill="none" stroke="currentColor" stroke-width="2">
                                        <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path>
                                        <polyline points="15 3 21 3 21 9"></polyline>
                                        <line x1="10" y1="14" x2="21" y2="3"></line>
                                    </svg>
                                    새 탭에서 PDF 열기
                                </a>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </main>

    <footer class="site-footer">
        <div class="container">
            <p>&copy; 2025 PaperCast. Powered by Hugging Face, Gemini Pro, and Google TTS.</p>
        </div>
    </footer>

    <script>
        const papersData = [
  {
    "id": "2510.07581",
    "title": "Expanding the Action Space of LLMs to Reason Beyond Language",
    "authors": [
      "Zhongqi Yue",
      "Weishi Wang",
      "Yundaichuan Zhan",
      "Juncheng Li",
      "Daniel Dahlmeier",
      "Fredrik D. Johansson"
    ],
    "abstract": "Large Language Models (LLMs) are powerful reasoners in natural language, but\ntheir actions are typically confined to outputting vocabulary tokens. As a\nresult, interactions with external environments -- such as symbolic operators\nor simulators -- must be expressed through text in predefined formats, parsed,\nand routed to external interfaces. This overloads the model's language with\nboth reasoning and control duties, and requires a hand-crafted parser, external\nto the LLM. To address this, we decouple environment interactions from language\nby internalizing them in an Expanded Action space (ExpA), beyond the\nvocabulary. The model starts reasoning in the default language environment, but\nmay trigger routing actions and switch to an external environment at any time.\nFrom there, the model can only invoke environment-specific actions, receive\nfeedback from the environment, and potentially route back to language as a\nresult. To promote effective exploration of the expanded action space and new\nenvironments, we introduce ExpA Reinforcement Learning (EARL) with\ncounterfactual policy optimization. On tasks requiring multi-turn interactions\nand contingent planning, EARL outperforms strong baselines with\nvocabulary-constrained actions. It performs robustly across calculator-based\nmulti-task learning and, in the partially observed sorting problem, achieves\nperfect Sort-4 accuracy while self-discovering an efficient algorithm\ncompetitive with classical designs.",
    "url": "https://huggingface.co/papers/2510.07581",
    "published_date": "2025-10-08",
    "upvotes": 1,
    "summary": "이 논문은 대규모 언어 모델(LLM)이 자연어 추론에 강력하지만, 그 행동이 어휘 토큰 출력에 국한되어 외부 환경과의 상호작용에 비효율적이라는 문제점을 지적합니다. 기존 방식은 외부 환경(예: 계산기, 시뮬레이터)과 상호작용하기 위해 명령어를 미리 정의된 텍스트 형식으로 표현하고, 이를 외부 파서가 분석하여 라우팅해야 하므로, LLM의 언어 모델에 추론과 제어라는 이중 부담을 지우고 외부 파서가 필요하다는 한계를 가집니다.\n\n**주요 내용:**\n이러한 한계를 극복하기 위해 저자들은 '확장된 행동 공간(Expanded Action space, ExpA)'을 제안합니다. ExpA는 외부 환경과의 상호작용을 언어로부터 분리하여 LLM 내부에 통합함으로써, LLM이 언어 토큰을 넘어선 행동을 수행할 수 있도록 합니다. 모델은 기본 언어 환경에서 추론을 시작하지만, 필요에 따라 '라우팅 액션'을 통해 언제든지 외부 환경으로 전환할 수 있습니다. 외부 환경에서는 해당 환경에 특화된 액션만 호출할 수 있으며, 환경으로부터 피드백을 받아 다시 언어 환경으로 복귀할 수도 있습니다. 확장된 행동 공간과 새로운 환경을 효과적으로 탐색하기 위해, 논문은 반사실적 정책 최적화(counterfactual policy optimization)를 포함하는 'ExpA 강화 학습(ExpA Reinforcement Learning, EARL)'이라는 방법론을 도입합니다.\n\n**핵심 기여점:**\n1.  **확장된 행동 공간(ExpA) 개념 도입:** LLM의 행동 영역을 단순한 언어 토큰 출력에서 벗어나 외부 환경 상호작용까지 확장하여, 언어 추론과 환경 제어의 역할을 분리함으로써 LLM의 유연성과 효율성을 크게 향상시켰습니다. 이는 LLM이 외부 인터페이스와의 복잡한 상호작용을 내부적으로 처리할 수 있게 하여, 수동으로 제작해야 했던 외부 파서의 필요성을 없앱니다.\n2.  **ExpA 강화 학습(EARL) 방법론 개발:** 확장된 행동 공간과 새로운 환경을 효과적으로 탐색하고 학습하기 위한 'ExpA 강화 학습(ExpA Reinforcement Learning, EARL)'과 반사실적 정책 최적화 기법을 제시했습니다. 이는 LLM이 언어와 외부 환경 사이를 오가며 복잡한 다단계 작업을 수행하도록 훈련하는 데 핵심적인 역할을 합니다.\n\n**중요한 결과 및 발견:**\nEARL은 다중 턴 상호작용과 조건부 계획이 필요한 복잡한 작업에서 어휘 기반 행동에 국한된 강력한 기준선 모델들을 능가하는 성능을 보여줍니다. 특히, 계산기 기반 다중 작업 학습에서 견고한 성능을 발휘했으며, 부분적으로 관찰되는 정렬 문제(Sort-4)에서는 완벽한 정확도를 달성했습니다. 더욱 주목할 만한 점은 LLM이 고전적인 설계에 필적하는 효율적인 정렬 알고리즘을 스스로 발견했다는 것입니다. 이는 LLM이 언어적 추론을 넘어선 복잡한 문제 해결 능력과 알고리즘 발견 잠재력을 가짐을 시사하며, 외부 도구 사용을 넘어선 새로운 형태의 지능을 보여줍니다.",
    "collected_at": "2025-10-22T16:17:55.356751",
    "arxiv_id": null,
    "categories": null,
    "thumbnail_url": null,
    "embed_supported": null,
    "view_count": null
  },
  {
    "id": "2510.15710",
    "title": "Unimedvl: Unifying Medical Multimodal Understanding And Generation\n  Through Observation-Knowledge-Analysis",
    "authors": [
      "Junzhi Ning",
      "Wei Li",
      "Cheng Tang",
      "Jiashi Lin",
      "Chenglong Ma",
      "Chaoyang Zhang",
      "Jiyao Liu",
      "Ying Chen",
      "Shujian Gao",
      "Lihao Liu",
      "Yuandong Pu",
      "Huihui Xu",
      "Chenhui Gou",
      "Ziyan Huang",
      "Yi Xin",
      "Qi Qin",
      "Zhongying Deng",
      "Diping Song",
      "Bin Fu",
      "Guang Yang",
      "Yuanfeng Ji",
      "Tianbin Li",
      "Yanzhou Su",
      "Jin Ye",
      "Shixiang Tang",
      "Ming Hu",
      "Junjun He"
    ],
    "abstract": "Medical diagnostic applications require models that can process multimodal\nmedical inputs (images, patient histories, lab results) and generate diverse\noutputs including both textual reports and visual content (annotations,\nsegmentation masks, and images). Despite this need, existing medical AI systems\ndisrupt this unified process: medical image understanding models interpret\nimages but cannot generate visual outputs, while medical image generation\nmodels synthesize images but cannot provide textual explanations. This leads to\ngaps in data representation, feature integration, and task-level multimodal\ncapabilities. To this end, we propose a multi-level framework that draws\ninspiration from diagnostic workflows through the\nObservation-Knowledge-Analysis (OKA) paradigm. Specifically, at the observation\nlevel, we construct UniMed-5M, a dataset comprising over 5.6M samples that\nreformat diverse unimodal data into multimodal pairs for foundational\nobservation. At the knowledge level, we propose Progressive Curriculum Learning\nthat systematically introduces medical multimodal knowledge. At the analysis\nlevel, we introduce UniMedVL, the first medical unified multimodal model for\nthe simultaneous analysis of image understanding and generation tasks within a\nsingle architecture. UniMedVL achieves superior performance on five medical\nimage understanding benchmarks, while matching specialized models in generation\nquality across eight medical imaging modalities. Crucially, our unified\narchitecture enables bidirectional knowledge sharing: generation tasks enhance\nvisual understanding features, demonstrating that integrating traditionally\nseparate capabilities within a single medical framework unlocks improvements\nacross diverse medical vision-language tasks. Code is available at\nhttps://github.com/uni-medical/UniMedVL.",
    "url": "https://huggingface.co/papers/2510.15710",
    "published_date": "2025-10-17",
    "upvotes": 1,
    "summary": "이 논문은 의료 진단 애플리케이션이 다중 모달 의료 입력(이미지, 환자 이력, 검사 결과)을 처리하고 텍스트 보고서와 시각적 콘텐츠(주석, 분할 마스크, 이미지)를 포함한 다양한 출력을 생성할 수 있는 모델을 필요로 한다는 점에 주목합니다. 그러나 기존 의료 AI 시스템들은 이러한 통합된 과정을 방해합니다. 의료 영상 이해 모델은 이미지를 해석하지만 시각적 출력을 생성할 수 없고, 의료 영상 생성 모델은 이미지를 합성하지만 텍스트 설명을 제공할 수 없습니다. 이는 데이터 표현, 특징 통합, 그리고 작업 수준의 다중 모달 기능에 간극을 초래합니다.\n\n이러한 문제 해결을 위해, 저자들은 진단 워크플로우에서 영감을 받은 '관찰-지식-분석(Observation-Knowledge-Analysis, OKA)' 패러다임을 통해 다단계 프레임워크를 제안합니다.\n\n**핵심 기여점:**\n\n1.  **관찰(Observation) 수준:** 다양한 단일 모달 데이터를 다중 모달 쌍으로 재구성하여 기본적인 관찰을 위한 **UniMed-5M** 데이터셋을 구축했습니다. 이 데이터셋은 560만 개 이상의 샘플로 구성되어 있습니다.\n2.  **지식(Knowledge) 수준:** 의료 다중 모달 지식을 체계적으로 도입하는 **점진적 커리큘럼 학습(Progressive Curriculum Learning)** 방법을 제안합니다.\n3.  **분석(Analysis) 수준:** 단일 아키텍처 내에서 영상 이해 및 생성 작업을 동시에 분석하는 최초의 의료 통합 다중 모달 모델인 **UniMedVL**을 소개합니다.\n\n**중요한 결과 및 발견:**\n\n*   **성능 우수성:** UniMedVL은 5개의 의료 영상 이해 벤치마크에서 우수한 성능을 달성했으며, 8개 의료 영상 모달리티 전반에 걸쳐 생성 품질에서 전문화된 모델들과 동등한 수준을 보였습니다.\n*   **양방향 지식 공유:** 가장 중요한 발견은 통합 아키텍처가 **양방향 지식 공유**를 가능하게 한다는 점입니다. 즉, 생성 작업이 시각적 이해 특징을 향상시키는 것을 보여주며, 이는 전통적으로 분리되어 있던 기능들을 단일 의료 프레임워크 내에 통합하는 것이 다양한 의료 시각-언어 작업 전반에 걸쳐 성능 개선을 이끌어낸다는 것을 입증합니다.\n\n결론적으로, UniMedVL은 의료 분야에서 다중 모달 이해와 생성을 통합하는 혁신적인 접근 방식을 제시하며, 분리된 AI 시스템의 한계를 극복하고 의료 진단 애플리케이션의 발전에 기여할 잠재력을 보여줍니다.",
    "collected_at": "2025-10-22T16:17:55.356866",
    "arxiv_id": null,
    "categories": null,
    "thumbnail_url": null,
    "embed_supported": null,
    "view_count": null
  },
  {
    "id": "2510.18632",
    "title": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from\n  Limited Views",
    "authors": [
      "Zhangquan Chen",
      "Manyuan Zhang",
      "Xinlei Yu",
      "Xufang Luo",
      "Mingze Sun",
      "Zihao Pan",
      "Yan Feng",
      "Peng Pei",
      "Xunliang Cai",
      "Ruqi Huang"
    ],
    "abstract": "Though recent advances in vision-language models (VLMs) have achieved\nremarkable progress across a wide range of multimodal tasks, understanding 3D\nspatial relationships from limited views remains a significant challenge.\nPrevious reasoning methods typically rely on pure text (e.g., topological\ncognitive maps) or on 2D visual cues. However, their limited representational\ncapacity hinders performance in specific tasks that require 3D spatial\nimagination. To address this limitation, we propose 3DThinker, a framework that\ncan effectively exploits the rich geometric information embedded within images\nwhile reasoning, like humans do. Our framework is the first to enable 3D\nmentaling during reasoning without any 3D prior input, and it does not rely on\nexplicitly labeled 3D data for training. Specifically, our training consists of\ntwo stages. First, we perform supervised training to align the 3D latent\ngenerated by VLM while reasoning with that of a 3D foundation model (e.g.,\nVGGT). Then, we optimize the entire reasoning trajectory solely based on\noutcome signals, thereby refining the underlying 3D mentaling. Extensive\nexperiments across multiple benchmarks show that 3DThinker consistently\noutperforms strong baselines and offers a new perspective toward unifying 3D\nrepresentations into multimodal reasoning. Our code will be available at\nhttps://github.com/zhangquanchen/3DThinker.",
    "url": "https://huggingface.co/papers/2510.18632",
    "published_date": "2025-10-21",
    "upvotes": 1,
    "summary": "다음은 논문 \"Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views\"의 한국어 요약입니다.\n\n**요약:**\n\n이 논문은 시각-언어 모델(VLMs)이 제한된 시야에서 3D 공간 관계를 이해하는 데 겪는 어려움을 해결하기 위해 **3DThinker** 프레임워크를 제안합니다. 기존 추론 방식은 순수 텍스트나 2D 시각적 단서에 의존하여 3D 공간 상상력을 요구하는 작업에서 한계를 보였습니다. 3DThinker는 인간처럼 이미지 내의 풍부한 기하학적 정보를 활용하여 3D 공간 추론을 수행하며, 특히 3D 사전 입력이나 명시적으로 레이블링된 3D 훈련 데이터 없이도 추론 과정에서 \"3D 멘탈링(3D 공간 상상력)\"을 가능하게 하는 최초의 프레임워크입니다.\n\n**1. 논문의 주요 내용:**\n3DThinker의 훈련은 두 단계로 진행됩니다. 첫째, 지도 학습을 통해 VLM이 추론 중 생성하는 3D 잠재 공간을 3D 파운데이션 모델(예: VGGT)의 잠재 공간과 정렬합니다. 둘째, 결과 신호(outcome signals)만을 기반으로 전체 추론 궤적을 최적화하여 3D 멘탈링 능력을 정교하게 다듬습니다. 이 접근 방식은 3D 데이터의 명시적인 입력이나 레이블링 없이도 모델이 3D 공간을 상상하고 추론할 수 있도록 합니다.\n\n**2. 핵심 기여점:**\n*   **3D 데이터 없는 3D 멘탈링:** 3D 사전 지식이나 명시적인 3D 레이블 데이터 없이도 VLM이 3D 공간 상상력을 수행하도록 하는 최초의 프레임워크를 제시하여 3D 데이터 제약을 극복했습니다.\n*   **혁신적인 2단계 훈련 전략:** VLM의 3D 잠재 공간을 3D 파운데이션 모델과 정렬한 후, 결과 신호 기반으로 추론 경로를 최적화하는 독창적인 훈련 방식을 제안했습니다.\n*   **멀티모달 추론에 3D 표현 통합:** 3D 표현을 멀티모달 추론에 효과적으로 통합하는 새로운 관점을 제시하여, VLM의 3D 공간 이해 능력 향상에 기여했습니다.\n\n**3. 중요한 결과나 발견:**\n다수의 벤치마크 실험 결과, 3DThinker는 강력한 기존 베이스라인 모델들을 일관되게 능가하는 성능을 보였습니다. 이는 이미지를 통해 얻은 풍부한 기하학적 정보가 3D 공간 추론을 효과적으로 강화할 수 있음을 입증하며, 명시적인 3D 데이터 없이도 VLM 내에서 3D 공간 상상력을 유도할 수 있다는 중요한 발견을 제시합니다. 이러한 성과는 제한된 뷰에서 3D 공간 관계를 이해하고 추론하는 데 있어 상당한 발전을 의미합니다.",
    "collected_at": "2025-10-22T16:17:55.356888",
    "arxiv_id": null,
    "categories": null,
    "thumbnail_url": null,
    "embed_supported": null,
    "view_count": null
  }
];
    </script>
    <script src="../assets/js/script.js"></script>
</body>
</html>