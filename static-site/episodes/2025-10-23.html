<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily AI Papers - 2025-10-23 - PaperCast</title>
    <meta name="description" content="ì˜¤ëŠ˜ì˜ Hugging Face íŠ¸ë Œë”© ë…¼ë¬¸ Top 3">
    <link rel="stylesheet" href="../assets/css/styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@300;400;500;700&display=swap" rel="stylesheet">
</head>
<body>
    <header class="site-header episode-header">
        <div class="container">
            <nav class="breadcrumb">
                <a href="../index.html">ğŸ  í™ˆ</a> / <span>ì—í”¼ì†Œë“œ</span>
            </nav>
            <h1 class="episode-title">Daily AI Papers - 2025-10-23</h1>
            <p class="episode-date">2025ë…„ 10ì›” 22ì¼</p>
        </div>
    </header>

    <main class="episode-main">
        <div class="audio-player-section">
            <div class="container">
                <div class="audio-player-enhanced">
                    <audio id="podcast-audio" controls preload="metadata">
                        <source src="https://storage.googleapis.com/papers_ethan/2025-10-23/episode.mp3" type="audio/mpeg">
                        ì£„ì†¡í•©ë‹ˆë‹¤. ë¸Œë¼ìš°ì €ê°€ ì˜¤ë””ì˜¤ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
                    </audio>
                </div>
                
                <div class="episode-info">
                    <p class="episode-description">ì˜¤ëŠ˜ì˜ Hugging Face íŠ¸ë Œë”© ë…¼ë¬¸ Top 3</p>
                </div>
            </div>
        </div>

        <div class="papers-section">
            <div class="container">
                <div class="section-header">
                    <h2 class="section-title">ë…¼ë¬¸ ìƒì„¸ ì •ë³´</h2>
                    <button id="split-view-toggle" class="btn btn-secondary" aria-label="Split View í† ê¸€">
                        ğŸ”„ Split View
                    </button>
                </div>
                
                <div class="papers-grid" id="papers-grid">
                    
                <article class="paper-card" data-paper-id="2510.07581" data-paper-index="0">
                    <div class="paper-thumbnail">
                        <img src="../assets/images/placeholder.png" alt="Expanding the Action Space of LLMs to Reason Beyond Language thumbnail" loading="lazy">
                        
                    </div>
                    
                    <div class="paper-content">
                        <h3 class="paper-title">Expanding the Action Space of LLMs to Reason Beyond Language</h3>
                        <p class="paper-authors">Zhongqi Yue, Weishi Wang, Yundaichuan Zhan ì™¸ 3ëª…</p>
                        <p class="paper-abstract">Large Language Models (LLMs) are powerful reasoners in natural language, but
their actions are typically confined to outputting vocabulary tokens. As a
result, interactions with external environments ...</p>
                        
                        <div class="paper-meta">
                            <span class="upvotes">ğŸ‘ 1</span>
                            
                            <span class="paper-date">ğŸ“… 2025-10-08</span>
                        </div>
                    </div>
                    
                    <div class="paper-actions">
                        <button class="btn btn-primary view-paper-btn" 
                                data-paper-url="https://huggingface.co/papers/2510.07581" 
                                data-arxiv-id=""
                                onclick="openPaperPDF('', 'https://huggingface.co/papers/2510.07581')">
                            ğŸ“„ View PDF
                        </button>
                        <button class="btn btn-secondary split-view-btn" 
                                data-paper-id="2510.07581" 
                                data-paper-index="0"
                                onclick="toggleSplitView(0)">
                            ğŸ”„ Split View
                        </button>
                    </div>
                </article>
            

                <article class="paper-card" data-paper-id="2510.15710" data-paper-index="1">
                    <div class="paper-thumbnail">
                        <img src="../assets/images/placeholder.png" alt="Unimedvl: Unifying Medical Multimodal Understanding And Generation
  Through Observation-Knowledge-Analysis thumbnail" loading="lazy">
                        
                    </div>
                    
                    <div class="paper-content">
                        <h3 class="paper-title">Unimedvl: Unifying Medical Multimodal Understanding And Generation
  Through Observation-Knowledge-Analysis</h3>
                        <p class="paper-authors">Junzhi Ning, Wei Li, Cheng Tang ì™¸ 24ëª…</p>
                        <p class="paper-abstract">Medical diagnostic applications require models that can process multimodal
medical inputs (images, patient histories, lab results) and generate diverse
outputs including both textual reports and visua...</p>
                        
                        <div class="paper-meta">
                            <span class="upvotes">ğŸ‘ 1</span>
                            
                            <span class="paper-date">ğŸ“… 2025-10-17</span>
                        </div>
                    </div>
                    
                    <div class="paper-actions">
                        <button class="btn btn-primary view-paper-btn" 
                                data-paper-url="https://huggingface.co/papers/2510.15710" 
                                data-arxiv-id=""
                                onclick="openPaperPDF('', 'https://huggingface.co/papers/2510.15710')">
                            ğŸ“„ View PDF
                        </button>
                        <button class="btn btn-secondary split-view-btn" 
                                data-paper-id="2510.15710" 
                                data-paper-index="1"
                                onclick="toggleSplitView(1)">
                            ğŸ”„ Split View
                        </button>
                    </div>
                </article>
            

                <article class="paper-card" data-paper-id="2510.18632" data-paper-index="2">
                    <div class="paper-thumbnail">
                        <img src="../assets/images/placeholder.png" alt="Think with 3D: Geometric Imagination Grounded Spatial Reasoning from
  Limited Views thumbnail" loading="lazy">
                        
                    </div>
                    
                    <div class="paper-content">
                        <h3 class="paper-title">Think with 3D: Geometric Imagination Grounded Spatial Reasoning from
  Limited Views</h3>
                        <p class="paper-authors">Zhangquan Chen, Manyuan Zhang, Xinlei Yu ì™¸ 7ëª…</p>
                        <p class="paper-abstract">Though recent advances in vision-language models (VLMs) have achieved
remarkable progress across a wide range of multimodal tasks, understanding 3D
spatial relationships from limited views remains a s...</p>
                        
                        <div class="paper-meta">
                            <span class="upvotes">ğŸ‘ 1</span>
                            
                            <span class="paper-date">ğŸ“… 2025-10-21</span>
                        </div>
                    </div>
                    
                    <div class="paper-actions">
                        <button class="btn btn-primary view-paper-btn" 
                                data-paper-url="https://huggingface.co/papers/2510.18632" 
                                data-arxiv-id=""
                                onclick="openPaperPDF('', 'https://huggingface.co/papers/2510.18632')">
                            ğŸ“„ View PDF
                        </button>
                        <button class="btn btn-secondary split-view-btn" 
                                data-paper-id="2510.18632" 
                                data-paper-index="2"
                                onclick="toggleSplitView(2)">
                            ğŸ”„ Split View
                        </button>
                    </div>
                </article>
            
                </div>
            </div>
        </div>

        <!-- Split View Container -->
        <div id="split-view-container" class="split-view-container" data-active="false" aria-hidden="true">
            <div class="split-view-left">
                <div class="player-section-compact">
                    <h3>ì˜¤ë””ì˜¤ í”Œë ˆì´ì–´</h3>
                    <div id="audio-player-placeholder" class="audio-player-placeholder">
                        <p>Split View ëª¨ë“œì—ì„œëŠ” ìœ„ì˜ ì˜¤ë””ì˜¤ í”Œë ˆì´ì–´ê°€ ì´ê³³ìœ¼ë¡œ ì´ë™í•©ë‹ˆë‹¤.</p>
                    </div>
                </div>
            </div>
            
            <div class="split-view-divider" role="separator" aria-orientation="vertical">
                <div class="divider-handle"></div>
            </div>
            
            <div class="split-view-right">
                <div class="paper-viewer-section">
                    <div class="paper-viewer-header">
                        <h3 id="current-paper-title" class="current-paper-title"></h3>
                        <button id="close-split-view" class="close-button" aria-label="Split View ë‹«ê¸°">âœ•</button>
                    </div>
                    <div class="paper-viewer-content">
                        <iframe id="paper-embed" class="paper-embed" frameborder="0"></iframe>
                        <div id="pdf-viewer-container" class="pdf-viewer-container" style="display: none;">
                            <iframe id="pdf-viewer" class="pdf-viewer" frameborder="0"></iframe>
                        </div>
                        <div id="paper-fallback" class="paper-fallback" style="display: none;">
                            <div class="fallback-content">
                                <svg class="fallback-icon" viewBox="0 0 24 24" width="48" height="48" fill="none" stroke="currentColor" stroke-width="2">
                                    <path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path>
                                    <polyline points="14 2 14 8 20 8"></polyline>
                                    <line x1="12" y1="18" x2="12" y2="12"></line>
                                    <line x1="9" y1="15" x2="15" y2="15"></line>
                                </svg>
                                <p>PDF ë·°ì–´ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.</p>
                                <p class="fallback-description">ë¸Œë¼ìš°ì €ì—ì„œ ì§ì ‘ PDFë¥¼ ì—´ì–´ë³´ì„¸ìš”.</p>
                                <a id="fallback-link" href="#" target="_blank" rel="noopener noreferrer" class="btn btn-primary">
                                    <svg class="btn-icon" viewBox="0 0 24 24" width="20" height="20" fill="none" stroke="currentColor" stroke-width="2">
                                        <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path>
                                        <polyline points="15 3 21 3 21 9"></polyline>
                                        <line x1="10" y1="14" x2="21" y2="3"></line>
                                    </svg>
                                    ìƒˆ íƒ­ì—ì„œ PDF ì—´ê¸°
                                </a>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </main>

    <footer class="site-footer">
        <div class="container">
            <p>&copy; 2025 PaperCast. Powered by Hugging Face, Gemini Pro, and Google TTS.</p>
        </div>
    </footer>

    <script>
        const papersData = [
  {
    "id": "2510.07581",
    "title": "Expanding the Action Space of LLMs to Reason Beyond Language",
    "authors": [
      "Zhongqi Yue",
      "Weishi Wang",
      "Yundaichuan Zhan",
      "Juncheng Li",
      "Daniel Dahlmeier",
      "Fredrik D. Johansson"
    ],
    "abstract": "Large Language Models (LLMs) are powerful reasoners in natural language, but\ntheir actions are typically confined to outputting vocabulary tokens. As a\nresult, interactions with external environments -- such as symbolic operators\nor simulators -- must be expressed through text in predefined formats, parsed,\nand routed to external interfaces. This overloads the model's language with\nboth reasoning and control duties, and requires a hand-crafted parser, external\nto the LLM. To address this, we decouple environment interactions from language\nby internalizing them in an Expanded Action space (ExpA), beyond the\nvocabulary. The model starts reasoning in the default language environment, but\nmay trigger routing actions and switch to an external environment at any time.\nFrom there, the model can only invoke environment-specific actions, receive\nfeedback from the environment, and potentially route back to language as a\nresult. To promote effective exploration of the expanded action space and new\nenvironments, we introduce ExpA Reinforcement Learning (EARL) with\ncounterfactual policy optimization. On tasks requiring multi-turn interactions\nand contingent planning, EARL outperforms strong baselines with\nvocabulary-constrained actions. It performs robustly across calculator-based\nmulti-task learning and, in the partially observed sorting problem, achieves\nperfect Sort-4 accuracy while self-discovering an efficient algorithm\ncompetitive with classical designs.",
    "url": "https://huggingface.co/papers/2510.07581",
    "published_date": "2025-10-08",
    "upvotes": 1,
    "summary": "ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì´ ìì—°ì–´ ì¶”ë¡ ì— ê°•ë ¥í•˜ì§€ë§Œ, ê·¸ í–‰ë™ì´ ì–´íœ˜ í† í° ì¶œë ¥ì— êµ­í•œë˜ì–´ ì™¸ë¶€ í™˜ê²½ê³¼ì˜ ìƒí˜¸ì‘ìš©ì— ë¹„íš¨ìœ¨ì ì´ë¼ëŠ” ë¬¸ì œì ì„ ì§€ì í•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ì‹ì€ ì™¸ë¶€ í™˜ê²½(ì˜ˆ: ê³„ì‚°ê¸°, ì‹œë®¬ë ˆì´í„°)ê³¼ ìƒí˜¸ì‘ìš©í•˜ê¸° ìœ„í•´ ëª…ë ¹ì–´ë¥¼ ë¯¸ë¦¬ ì •ì˜ëœ í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ í‘œí˜„í•˜ê³ , ì´ë¥¼ ì™¸ë¶€ íŒŒì„œê°€ ë¶„ì„í•˜ì—¬ ë¼ìš°íŒ…í•´ì•¼ í•˜ë¯€ë¡œ, LLMì˜ ì–¸ì–´ ëª¨ë¸ì— ì¶”ë¡ ê³¼ ì œì–´ë¼ëŠ” ì´ì¤‘ ë¶€ë‹´ì„ ì§€ìš°ê³  ì™¸ë¶€ íŒŒì„œê°€ í•„ìš”í•˜ë‹¤ëŠ” í•œê³„ë¥¼ ê°€ì§‘ë‹ˆë‹¤.\n\n**ì£¼ìš” ë‚´ìš©:**\nì´ëŸ¬í•œ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ì €ìë“¤ì€ 'í™•ì¥ëœ í–‰ë™ ê³µê°„(Expanded Action space, ExpA)'ì„ ì œì•ˆí•©ë‹ˆë‹¤. ExpAëŠ” ì™¸ë¶€ í™˜ê²½ê³¼ì˜ ìƒí˜¸ì‘ìš©ì„ ì–¸ì–´ë¡œë¶€í„° ë¶„ë¦¬í•˜ì—¬ LLM ë‚´ë¶€ì— í†µí•©í•¨ìœ¼ë¡œì¨, LLMì´ ì–¸ì–´ í† í°ì„ ë„˜ì–´ì„  í–‰ë™ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ëª¨ë¸ì€ ê¸°ë³¸ ì–¸ì–´ í™˜ê²½ì—ì„œ ì¶”ë¡ ì„ ì‹œì‘í•˜ì§€ë§Œ, í•„ìš”ì— ë”°ë¼ 'ë¼ìš°íŒ… ì•¡ì…˜'ì„ í†µí•´ ì–¸ì œë“ ì§€ ì™¸ë¶€ í™˜ê²½ìœ¼ë¡œ ì „í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì™¸ë¶€ í™˜ê²½ì—ì„œëŠ” í•´ë‹¹ í™˜ê²½ì— íŠ¹í™”ëœ ì•¡ì…˜ë§Œ í˜¸ì¶œí•  ìˆ˜ ìˆìœ¼ë©°, í™˜ê²½ìœ¼ë¡œë¶€í„° í”¼ë“œë°±ì„ ë°›ì•„ ë‹¤ì‹œ ì–¸ì–´ í™˜ê²½ìœ¼ë¡œ ë³µê·€í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. í™•ì¥ëœ í–‰ë™ ê³µê°„ê³¼ ìƒˆë¡œìš´ í™˜ê²½ì„ íš¨ê³¼ì ìœ¼ë¡œ íƒìƒ‰í•˜ê¸° ìœ„í•´, ë…¼ë¬¸ì€ ë°˜ì‚¬ì‹¤ì  ì •ì±… ìµœì í™”(counterfactual policy optimization)ë¥¼ í¬í•¨í•˜ëŠ” 'ExpA ê°•í™” í•™ìŠµ(ExpA Reinforcement Learning, EARL)'ì´ë¼ëŠ” ë°©ë²•ë¡ ì„ ë„ì…í•©ë‹ˆë‹¤.\n\n**í•µì‹¬ ê¸°ì—¬ì :**\n1.  **í™•ì¥ëœ í–‰ë™ ê³µê°„(ExpA) ê°œë… ë„ì…:** LLMì˜ í–‰ë™ ì˜ì—­ì„ ë‹¨ìˆœí•œ ì–¸ì–´ í† í° ì¶œë ¥ì—ì„œ ë²—ì–´ë‚˜ ì™¸ë¶€ í™˜ê²½ ìƒí˜¸ì‘ìš©ê¹Œì§€ í™•ì¥í•˜ì—¬, ì–¸ì–´ ì¶”ë¡ ê³¼ í™˜ê²½ ì œì–´ì˜ ì—­í• ì„ ë¶„ë¦¬í•¨ìœ¼ë¡œì¨ LLMì˜ ìœ ì—°ì„±ê³¼ íš¨ìœ¨ì„±ì„ í¬ê²Œ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. ì´ëŠ” LLMì´ ì™¸ë¶€ ì¸í„°í˜ì´ìŠ¤ì™€ì˜ ë³µì¡í•œ ìƒí˜¸ì‘ìš©ì„ ë‚´ë¶€ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ í•˜ì—¬, ìˆ˜ë™ìœ¼ë¡œ ì œì‘í•´ì•¼ í–ˆë˜ ì™¸ë¶€ íŒŒì„œì˜ í•„ìš”ì„±ì„ ì—†ì•±ë‹ˆë‹¤.\n2.  **ExpA ê°•í™” í•™ìŠµ(EARL) ë°©ë²•ë¡  ê°œë°œ:** í™•ì¥ëœ í–‰ë™ ê³µê°„ê³¼ ìƒˆë¡œìš´ í™˜ê²½ì„ íš¨ê³¼ì ìœ¼ë¡œ íƒìƒ‰í•˜ê³  í•™ìŠµí•˜ê¸° ìœ„í•œ 'ExpA ê°•í™” í•™ìŠµ(ExpA Reinforcement Learning, EARL)'ê³¼ ë°˜ì‚¬ì‹¤ì  ì •ì±… ìµœì í™” ê¸°ë²•ì„ ì œì‹œí–ˆìŠµë‹ˆë‹¤. ì´ëŠ” LLMì´ ì–¸ì–´ì™€ ì™¸ë¶€ í™˜ê²½ ì‚¬ì´ë¥¼ ì˜¤ê°€ë©° ë³µì¡í•œ ë‹¤ë‹¨ê³„ ì‘ì—…ì„ ìˆ˜í–‰í•˜ë„ë¡ í›ˆë ¨í•˜ëŠ” ë° í•µì‹¬ì ì¸ ì—­í• ì„ í•©ë‹ˆë‹¤.\n\n**ì¤‘ìš”í•œ ê²°ê³¼ ë° ë°œê²¬:**\nEARLì€ ë‹¤ì¤‘ í„´ ìƒí˜¸ì‘ìš©ê³¼ ì¡°ê±´ë¶€ ê³„íšì´ í•„ìš”í•œ ë³µì¡í•œ ì‘ì—…ì—ì„œ ì–´íœ˜ ê¸°ë°˜ í–‰ë™ì— êµ­í•œëœ ê°•ë ¥í•œ ê¸°ì¤€ì„  ëª¨ë¸ë“¤ì„ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. íŠ¹íˆ, ê³„ì‚°ê¸° ê¸°ë°˜ ë‹¤ì¤‘ ì‘ì—… í•™ìŠµì—ì„œ ê²¬ê³ í•œ ì„±ëŠ¥ì„ ë°œíœ˜í–ˆìœ¼ë©°, ë¶€ë¶„ì ìœ¼ë¡œ ê´€ì°°ë˜ëŠ” ì •ë ¬ ë¬¸ì œ(Sort-4)ì—ì„œëŠ” ì™„ë²½í•œ ì •í™•ë„ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ë”ìš± ì£¼ëª©í•  ë§Œí•œ ì ì€ LLMì´ ê³ ì „ì ì¸ ì„¤ê³„ì— í•„ì í•˜ëŠ” íš¨ìœ¨ì ì¸ ì •ë ¬ ì•Œê³ ë¦¬ì¦˜ì„ ìŠ¤ìŠ¤ë¡œ ë°œê²¬í–ˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ëŠ” LLMì´ ì–¸ì–´ì  ì¶”ë¡ ì„ ë„˜ì–´ì„  ë³µì¡í•œ ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ê³¼ ì•Œê³ ë¦¬ì¦˜ ë°œê²¬ ì ì¬ë ¥ì„ ê°€ì§ì„ ì‹œì‚¬í•˜ë©°, ì™¸ë¶€ ë„êµ¬ ì‚¬ìš©ì„ ë„˜ì–´ì„  ìƒˆë¡œìš´ í˜•íƒœì˜ ì§€ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.",
    "collected_at": "2025-10-22T16:17:55.356751",
    "arxiv_id": null,
    "categories": null,
    "thumbnail_url": null,
    "embed_supported": null,
    "view_count": null
  },
  {
    "id": "2510.15710",
    "title": "Unimedvl: Unifying Medical Multimodal Understanding And Generation\n  Through Observation-Knowledge-Analysis",
    "authors": [
      "Junzhi Ning",
      "Wei Li",
      "Cheng Tang",
      "Jiashi Lin",
      "Chenglong Ma",
      "Chaoyang Zhang",
      "Jiyao Liu",
      "Ying Chen",
      "Shujian Gao",
      "Lihao Liu",
      "Yuandong Pu",
      "Huihui Xu",
      "Chenhui Gou",
      "Ziyan Huang",
      "Yi Xin",
      "Qi Qin",
      "Zhongying Deng",
      "Diping Song",
      "Bin Fu",
      "Guang Yang",
      "Yuanfeng Ji",
      "Tianbin Li",
      "Yanzhou Su",
      "Jin Ye",
      "Shixiang Tang",
      "Ming Hu",
      "Junjun He"
    ],
    "abstract": "Medical diagnostic applications require models that can process multimodal\nmedical inputs (images, patient histories, lab results) and generate diverse\noutputs including both textual reports and visual content (annotations,\nsegmentation masks, and images). Despite this need, existing medical AI systems\ndisrupt this unified process: medical image understanding models interpret\nimages but cannot generate visual outputs, while medical image generation\nmodels synthesize images but cannot provide textual explanations. This leads to\ngaps in data representation, feature integration, and task-level multimodal\ncapabilities. To this end, we propose a multi-level framework that draws\ninspiration from diagnostic workflows through the\nObservation-Knowledge-Analysis (OKA) paradigm. Specifically, at the observation\nlevel, we construct UniMed-5M, a dataset comprising over 5.6M samples that\nreformat diverse unimodal data into multimodal pairs for foundational\nobservation. At the knowledge level, we propose Progressive Curriculum Learning\nthat systematically introduces medical multimodal knowledge. At the analysis\nlevel, we introduce UniMedVL, the first medical unified multimodal model for\nthe simultaneous analysis of image understanding and generation tasks within a\nsingle architecture. UniMedVL achieves superior performance on five medical\nimage understanding benchmarks, while matching specialized models in generation\nquality across eight medical imaging modalities. Crucially, our unified\narchitecture enables bidirectional knowledge sharing: generation tasks enhance\nvisual understanding features, demonstrating that integrating traditionally\nseparate capabilities within a single medical framework unlocks improvements\nacross diverse medical vision-language tasks. Code is available at\nhttps://github.com/uni-medical/UniMedVL.",
    "url": "https://huggingface.co/papers/2510.15710",
    "published_date": "2025-10-17",
    "upvotes": 1,
    "summary": "ì´ ë…¼ë¬¸ì€ ì˜ë£Œ ì§„ë‹¨ ì• í”Œë¦¬ì¼€ì´ì…˜ì´ ë‹¤ì¤‘ ëª¨ë‹¬ ì˜ë£Œ ì…ë ¥(ì´ë¯¸ì§€, í™˜ì ì´ë ¥, ê²€ì‚¬ ê²°ê³¼)ì„ ì²˜ë¦¬í•˜ê³  í…ìŠ¤íŠ¸ ë³´ê³ ì„œì™€ ì‹œê°ì  ì½˜í…ì¸ (ì£¼ì„, ë¶„í•  ë§ˆìŠ¤í¬, ì´ë¯¸ì§€)ë¥¼ í¬í•¨í•œ ë‹¤ì–‘í•œ ì¶œë ¥ì„ ìƒì„±í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì„ í•„ìš”ë¡œ í•œë‹¤ëŠ” ì ì— ì£¼ëª©í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê¸°ì¡´ ì˜ë£Œ AI ì‹œìŠ¤í…œë“¤ì€ ì´ëŸ¬í•œ í†µí•©ëœ ê³¼ì •ì„ ë°©í•´í•©ë‹ˆë‹¤. ì˜ë£Œ ì˜ìƒ ì´í•´ ëª¨ë¸ì€ ì´ë¯¸ì§€ë¥¼ í•´ì„í•˜ì§€ë§Œ ì‹œê°ì  ì¶œë ¥ì„ ìƒì„±í•  ìˆ˜ ì—†ê³ , ì˜ë£Œ ì˜ìƒ ìƒì„± ëª¨ë¸ì€ ì´ë¯¸ì§€ë¥¼ í•©ì„±í•˜ì§€ë§Œ í…ìŠ¤íŠ¸ ì„¤ëª…ì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ëŠ” ë°ì´í„° í‘œí˜„, íŠ¹ì§• í†µí•©, ê·¸ë¦¬ê³  ì‘ì—… ìˆ˜ì¤€ì˜ ë‹¤ì¤‘ ëª¨ë‹¬ ê¸°ëŠ¥ì— ê°„ê·¹ì„ ì´ˆë˜í•©ë‹ˆë‹¤.\n\nì´ëŸ¬í•œ ë¬¸ì œ í•´ê²°ì„ ìœ„í•´, ì €ìë“¤ì€ ì§„ë‹¨ ì›Œí¬í”Œë¡œìš°ì—ì„œ ì˜ê°ì„ ë°›ì€ 'ê´€ì°°-ì§€ì‹-ë¶„ì„(Observation-Knowledge-Analysis, OKA)' íŒ¨ëŸ¬ë‹¤ì„ì„ í†µí•´ ë‹¤ë‹¨ê³„ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.\n\n**í•µì‹¬ ê¸°ì—¬ì :**\n\n1.  **ê´€ì°°(Observation) ìˆ˜ì¤€:** ë‹¤ì–‘í•œ ë‹¨ì¼ ëª¨ë‹¬ ë°ì´í„°ë¥¼ ë‹¤ì¤‘ ëª¨ë‹¬ ìŒìœ¼ë¡œ ì¬êµ¬ì„±í•˜ì—¬ ê¸°ë³¸ì ì¸ ê´€ì°°ì„ ìœ„í•œ **UniMed-5M** ë°ì´í„°ì…‹ì„ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤. ì´ ë°ì´í„°ì…‹ì€ 560ë§Œ ê°œ ì´ìƒì˜ ìƒ˜í”Œë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n2.  **ì§€ì‹(Knowledge) ìˆ˜ì¤€:** ì˜ë£Œ ë‹¤ì¤‘ ëª¨ë‹¬ ì§€ì‹ì„ ì²´ê³„ì ìœ¼ë¡œ ë„ì…í•˜ëŠ” **ì ì§„ì  ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ(Progressive Curriculum Learning)** ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.\n3.  **ë¶„ì„(Analysis) ìˆ˜ì¤€:** ë‹¨ì¼ ì•„í‚¤í…ì²˜ ë‚´ì—ì„œ ì˜ìƒ ì´í•´ ë° ìƒì„± ì‘ì—…ì„ ë™ì‹œì— ë¶„ì„í•˜ëŠ” ìµœì´ˆì˜ ì˜ë£Œ í†µí•© ë‹¤ì¤‘ ëª¨ë‹¬ ëª¨ë¸ì¸ **UniMedVL**ì„ ì†Œê°œí•©ë‹ˆë‹¤.\n\n**ì¤‘ìš”í•œ ê²°ê³¼ ë° ë°œê²¬:**\n\n*   **ì„±ëŠ¥ ìš°ìˆ˜ì„±:** UniMedVLì€ 5ê°œì˜ ì˜ë£Œ ì˜ìƒ ì´í•´ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìœ¼ë©°, 8ê°œ ì˜ë£Œ ì˜ìƒ ëª¨ë‹¬ë¦¬í‹° ì „ë°˜ì— ê±¸ì³ ìƒì„± í’ˆì§ˆì—ì„œ ì „ë¬¸í™”ëœ ëª¨ë¸ë“¤ê³¼ ë™ë“±í•œ ìˆ˜ì¤€ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.\n*   **ì–‘ë°©í–¥ ì§€ì‹ ê³µìœ :** ê°€ì¥ ì¤‘ìš”í•œ ë°œê²¬ì€ í†µí•© ì•„í‚¤í…ì²˜ê°€ **ì–‘ë°©í–¥ ì§€ì‹ ê³µìœ **ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ì¦‰, ìƒì„± ì‘ì—…ì´ ì‹œê°ì  ì´í•´ íŠ¹ì§•ì„ í–¥ìƒì‹œí‚¤ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ë©°, ì´ëŠ” ì „í†µì ìœ¼ë¡œ ë¶„ë¦¬ë˜ì–´ ìˆë˜ ê¸°ëŠ¥ë“¤ì„ ë‹¨ì¼ ì˜ë£Œ í”„ë ˆì„ì›Œí¬ ë‚´ì— í†µí•©í•˜ëŠ” ê²ƒì´ ë‹¤ì–‘í•œ ì˜ë£Œ ì‹œê°-ì–¸ì–´ ì‘ì—… ì „ë°˜ì— ê±¸ì³ ì„±ëŠ¥ ê°œì„ ì„ ì´ëŒì–´ë‚¸ë‹¤ëŠ” ê²ƒì„ ì…ì¦í•©ë‹ˆë‹¤.\n\nê²°ë¡ ì ìœ¼ë¡œ, UniMedVLì€ ì˜ë£Œ ë¶„ì•¼ì—ì„œ ë‹¤ì¤‘ ëª¨ë‹¬ ì´í•´ì™€ ìƒì„±ì„ í†µí•©í•˜ëŠ” í˜ì‹ ì ì¸ ì ‘ê·¼ ë°©ì‹ì„ ì œì‹œí•˜ë©°, ë¶„ë¦¬ëœ AI ì‹œìŠ¤í…œì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³  ì˜ë£Œ ì§„ë‹¨ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ë°œì „ì— ê¸°ì—¬í•  ì ì¬ë ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.",
    "collected_at": "2025-10-22T16:17:55.356866",
    "arxiv_id": null,
    "categories": null,
    "thumbnail_url": null,
    "embed_supported": null,
    "view_count": null
  },
  {
    "id": "2510.18632",
    "title": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from\n  Limited Views",
    "authors": [
      "Zhangquan Chen",
      "Manyuan Zhang",
      "Xinlei Yu",
      "Xufang Luo",
      "Mingze Sun",
      "Zihao Pan",
      "Yan Feng",
      "Peng Pei",
      "Xunliang Cai",
      "Ruqi Huang"
    ],
    "abstract": "Though recent advances in vision-language models (VLMs) have achieved\nremarkable progress across a wide range of multimodal tasks, understanding 3D\nspatial relationships from limited views remains a significant challenge.\nPrevious reasoning methods typically rely on pure text (e.g., topological\ncognitive maps) or on 2D visual cues. However, their limited representational\ncapacity hinders performance in specific tasks that require 3D spatial\nimagination. To address this limitation, we propose 3DThinker, a framework that\ncan effectively exploits the rich geometric information embedded within images\nwhile reasoning, like humans do. Our framework is the first to enable 3D\nmentaling during reasoning without any 3D prior input, and it does not rely on\nexplicitly labeled 3D data for training. Specifically, our training consists of\ntwo stages. First, we perform supervised training to align the 3D latent\ngenerated by VLM while reasoning with that of a 3D foundation model (e.g.,\nVGGT). Then, we optimize the entire reasoning trajectory solely based on\noutcome signals, thereby refining the underlying 3D mentaling. Extensive\nexperiments across multiple benchmarks show that 3DThinker consistently\noutperforms strong baselines and offers a new perspective toward unifying 3D\nrepresentations into multimodal reasoning. Our code will be available at\nhttps://github.com/zhangquanchen/3DThinker.",
    "url": "https://huggingface.co/papers/2510.18632",
    "published_date": "2025-10-21",
    "upvotes": 1,
    "summary": "ë‹¤ìŒì€ ë…¼ë¬¸ \"Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views\"ì˜ í•œêµ­ì–´ ìš”ì•½ì…ë‹ˆë‹¤.\n\n**ìš”ì•½:**\n\nì´ ë…¼ë¬¸ì€ ì‹œê°-ì–¸ì–´ ëª¨ë¸(VLMs)ì´ ì œí•œëœ ì‹œì•¼ì—ì„œ 3D ê³µê°„ ê´€ê³„ë¥¼ ì´í•´í•˜ëŠ” ë° ê²ªëŠ” ì–´ë ¤ì›€ì„ í•´ê²°í•˜ê¸° ìœ„í•´ **3DThinker** í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ì¶”ë¡  ë°©ì‹ì€ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë‚˜ 2D ì‹œê°ì  ë‹¨ì„œì— ì˜ì¡´í•˜ì—¬ 3D ê³µê°„ ìƒìƒë ¥ì„ ìš”êµ¬í•˜ëŠ” ì‘ì—…ì—ì„œ í•œê³„ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤. 3DThinkerëŠ” ì¸ê°„ì²˜ëŸ¼ ì´ë¯¸ì§€ ë‚´ì˜ í’ë¶€í•œ ê¸°í•˜í•™ì  ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ 3D ê³µê°„ ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ë©°, íŠ¹íˆ 3D ì‚¬ì „ ì…ë ¥ì´ë‚˜ ëª…ì‹œì ìœ¼ë¡œ ë ˆì´ë¸”ë§ëœ 3D í›ˆë ¨ ë°ì´í„° ì—†ì´ë„ ì¶”ë¡  ê³¼ì •ì—ì„œ \"3D ë©˜íƒˆë§(3D ê³µê°„ ìƒìƒë ¥)\"ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ìµœì´ˆì˜ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n\n**1. ë…¼ë¬¸ì˜ ì£¼ìš” ë‚´ìš©:**\n3DThinkerì˜ í›ˆë ¨ì€ ë‘ ë‹¨ê³„ë¡œ ì§„í–‰ë©ë‹ˆë‹¤. ì²«ì§¸, ì§€ë„ í•™ìŠµì„ í†µí•´ VLMì´ ì¶”ë¡  ì¤‘ ìƒì„±í•˜ëŠ” 3D ì ì¬ ê³µê°„ì„ 3D íŒŒìš´ë°ì´ì…˜ ëª¨ë¸(ì˜ˆ: VGGT)ì˜ ì ì¬ ê³µê°„ê³¼ ì •ë ¬í•©ë‹ˆë‹¤. ë‘˜ì§¸, ê²°ê³¼ ì‹ í˜¸(outcome signals)ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ì „ì²´ ì¶”ë¡  ê¶¤ì ì„ ìµœì í™”í•˜ì—¬ 3D ë©˜íƒˆë§ ëŠ¥ë ¥ì„ ì •êµí•˜ê²Œ ë‹¤ë“¬ìŠµë‹ˆë‹¤. ì´ ì ‘ê·¼ ë°©ì‹ì€ 3D ë°ì´í„°ì˜ ëª…ì‹œì ì¸ ì…ë ¥ì´ë‚˜ ë ˆì´ë¸”ë§ ì—†ì´ë„ ëª¨ë¸ì´ 3D ê³µê°„ì„ ìƒìƒí•˜ê³  ì¶”ë¡ í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.\n\n**2. í•µì‹¬ ê¸°ì—¬ì :**\n*   **3D ë°ì´í„° ì—†ëŠ” 3D ë©˜íƒˆë§:** 3D ì‚¬ì „ ì§€ì‹ì´ë‚˜ ëª…ì‹œì ì¸ 3D ë ˆì´ë¸” ë°ì´í„° ì—†ì´ë„ VLMì´ 3D ê³µê°„ ìƒìƒë ¥ì„ ìˆ˜í–‰í•˜ë„ë¡ í•˜ëŠ” ìµœì´ˆì˜ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì‹œí•˜ì—¬ 3D ë°ì´í„° ì œì•½ì„ ê·¹ë³µí–ˆìŠµë‹ˆë‹¤.\n*   **í˜ì‹ ì ì¸ 2ë‹¨ê³„ í›ˆë ¨ ì „ëµ:** VLMì˜ 3D ì ì¬ ê³µê°„ì„ 3D íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ê³¼ ì •ë ¬í•œ í›„, ê²°ê³¼ ì‹ í˜¸ ê¸°ë°˜ìœ¼ë¡œ ì¶”ë¡  ê²½ë¡œë¥¼ ìµœì í™”í•˜ëŠ” ë…ì°½ì ì¸ í›ˆë ¨ ë°©ì‹ì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤.\n*   **ë©€í‹°ëª¨ë‹¬ ì¶”ë¡ ì— 3D í‘œí˜„ í†µí•©:** 3D í‘œí˜„ì„ ë©€í‹°ëª¨ë‹¬ ì¶”ë¡ ì— íš¨ê³¼ì ìœ¼ë¡œ í†µí•©í•˜ëŠ” ìƒˆë¡œìš´ ê´€ì ì„ ì œì‹œí•˜ì—¬, VLMì˜ 3D ê³µê°„ ì´í•´ ëŠ¥ë ¥ í–¥ìƒì— ê¸°ì—¬í–ˆìŠµë‹ˆë‹¤.\n\n**3. ì¤‘ìš”í•œ ê²°ê³¼ë‚˜ ë°œê²¬:**\në‹¤ìˆ˜ì˜ ë²¤ì¹˜ë§ˆí¬ ì‹¤í—˜ ê²°ê³¼, 3DThinkerëŠ” ê°•ë ¥í•œ ê¸°ì¡´ ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ë“¤ì„ ì¼ê´€ë˜ê²Œ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì´ëŠ” ì´ë¯¸ì§€ë¥¼ í†µí•´ ì–»ì€ í’ë¶€í•œ ê¸°í•˜í•™ì  ì •ë³´ê°€ 3D ê³µê°„ ì¶”ë¡ ì„ íš¨ê³¼ì ìœ¼ë¡œ ê°•í™”í•  ìˆ˜ ìˆìŒì„ ì…ì¦í•˜ë©°, ëª…ì‹œì ì¸ 3D ë°ì´í„° ì—†ì´ë„ VLM ë‚´ì—ì„œ 3D ê³µê°„ ìƒìƒë ¥ì„ ìœ ë„í•  ìˆ˜ ìˆë‹¤ëŠ” ì¤‘ìš”í•œ ë°œê²¬ì„ ì œì‹œí•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì„±ê³¼ëŠ” ì œí•œëœ ë·°ì—ì„œ 3D ê³µê°„ ê´€ê³„ë¥¼ ì´í•´í•˜ê³  ì¶”ë¡ í•˜ëŠ” ë° ìˆì–´ ìƒë‹¹í•œ ë°œì „ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.",
    "collected_at": "2025-10-22T16:17:55.356888",
    "arxiv_id": null,
    "categories": null,
    "thumbnail_url": null,
    "embed_supported": null,
    "view_count": null
  }
];
    </script>
    <script src="../assets/js/script.js"></script>
</body>
</html>