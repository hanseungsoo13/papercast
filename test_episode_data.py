#!/usr/bin/env python3
"""
GCSì— í…ŒìŠ¤íŠ¸ ì—í”¼ì†Œë“œ ë°ì´í„° ì—…ë¡œë“œ

ì‹¤ì œ ì—í”¼ì†Œë“œ ë°ì´í„°ë¥¼ GCSì— ì—…ë¡œë“œí•˜ì—¬ ë°±ì—”ë“œ API í…ŒìŠ¤íŠ¸
"""

import json
import sys
from datetime import datetime, timezone
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
PROJECT_ROOT = Path(__file__).parent
sys.path.insert(0, str(PROJECT_ROOT))

from src.services.uploader import GCSUploader
from src.utils.config import config


def create_test_episode_data():
    """í…ŒìŠ¤íŠ¸ìš© ì—í”¼ì†Œë“œ ë°ì´í„° ìƒì„±"""
    episode_id = "2025-01-23"
    
    # í…ŒìŠ¤íŠ¸ìš© ë…¼ë¬¸ ë°ì´í„°
    papers = [
        {
            "id": "test-paper-1",
            "title": "Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1",
            "authors": ["í…ŒìŠ¤íŠ¸ ì €ì 1", "í…ŒìŠ¤íŠ¸ ì €ì 2"],
            "abstract": "ì´ê²ƒì€ GCS í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ë”ë¯¸ ë…¼ë¬¸ ì´ˆë¡ì…ë‹ˆë‹¤. AIì™€ ì¸ê°„ì˜ í˜‘ì—…ì„ í†µí•œ ë¬¸ì„œ ì‘ì„±ì— ëŒ€í•œ ì—°êµ¬ì…ë‹ˆë‹¤.",
            "summary": "ì´ ë…¼ë¬¸ì€ AIì™€ ì¸ê°„ì˜ í˜‘ì—…ì„ í†µí•œ ë¬¸ì„œ ì‘ì„± ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. 0.1ë‹¬ëŸ¬ ë¯¸ë§Œì˜ ë¹„ìš©ìœ¼ë¡œ ê³ í’ˆì§ˆ ë¬¸ì„œë¥¼ ìƒì„±í•  ìˆ˜ ìˆëŠ” í˜ì‹ ì ì¸ ì ‘ê·¼ ë°©ì‹ì„ ì œì‹œí•©ë‹ˆë‹¤.",
            "short_summary": "AI-ì¸ê°„ í˜‘ì—… ë¬¸ì„œ ì‘ì„±\n0.1ë‹¬ëŸ¬ ë¯¸ë§Œ ë¹„ìš©ìœ¼ë¡œ ê³ í’ˆì§ˆ ë¬¸ì„œ ìƒì„±\ní˜ì‹ ì ì¸ ì›Œí¬í”Œë¡œìš° ì œì•ˆ",
            "url": "https://huggingface.co/papers/test-paper-1",
            "published_date": "2025-01-22",
            "upvotes": 150,
            "tags": ["AI", "Collaboration", "Document Generation"]
        },
        {
            "id": "test-paper-2", 
            "title": "Advanced Neural Network Architectures for Natural Language Processing",
            "authors": ["í…ŒìŠ¤íŠ¸ ì €ì 3", "í…ŒìŠ¤íŠ¸ ì €ì 4"],
            "abstract": "ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ ê³ ê¸‰ ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ì— ëŒ€í•œ ì—°êµ¬ì…ë‹ˆë‹¤. íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì˜ ê°œì„ ëœ ë²„ì „ì„ ì œì•ˆí•©ë‹ˆë‹¤.",
            "summary": "ì´ ë…¼ë¬¸ì€ ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ ìƒˆë¡œìš´ ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³  ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ëŠ” ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤.",
            "short_summary": "ìì—°ì–´ ì²˜ë¦¬ìš© ì‹ ê²½ë§ ì•„í‚¤í…ì²˜\níŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ ê°œì„ \nì„±ëŠ¥ í–¥ìƒ ë°©ë²• ì œì‹œ",
            "url": "https://huggingface.co/papers/test-paper-2",
            "published_date": "2025-01-21",
            "upvotes": 200,
            "tags": ["NLP", "Neural Networks", "Transformers"]
        },
        {
            "id": "test-paper-3",
            "title": "Efficient Training Methods for Large Language Models",
            "authors": ["í…ŒìŠ¤íŠ¸ ì €ì 5"],
            "abstract": "ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ íš¨ìœ¨ì ì¸ í›ˆë ¨ ë°©ë²•ì— ëŒ€í•œ ì—°êµ¬ì…ë‹ˆë‹¤. ê³„ì‚° ë¹„ìš©ì„ ì¤„ì´ë©´ì„œ ì„±ëŠ¥ì„ ìœ ì§€í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.",
            "summary": "ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ í›ˆë ¨ íš¨ìœ¨ì„±ì„ ê°œì„ í•˜ëŠ” ìƒˆë¡œìš´ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê³„ì‚° ë¹„ìš©ì„ í¬ê²Œ ì¤„ì´ë©´ì„œë„ ëª¨ë¸ ì„±ëŠ¥ì„ ìœ ì§€í•  ìˆ˜ ìˆëŠ” í˜ì‹ ì ì¸ í›ˆë ¨ ê¸°ë²•ì„ ì†Œê°œí•©ë‹ˆë‹¤.",
            "short_summary": "ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ í›ˆë ¨ íš¨ìœ¨ì„±\nê³„ì‚° ë¹„ìš© ì ˆê° ë°©ë²•\nì„±ëŠ¥ ìœ ì§€ ê¸°ë²•",
            "url": "https://huggingface.co/papers/test-paper-3",
            "published_date": "2025-01-20",
            "upvotes": 300,
            "tags": ["LLM", "Training", "Efficiency"]
        }
    ]
    
    # ì—í”¼ì†Œë“œ ë°ì´í„° ìƒì„±
    episode_data = {
        "id": episode_id,
        "title": f"Daily AI Papers - {episode_id}",
        "description": f"ì˜¤ëŠ˜ì˜ Hugging Face íŠ¸ë Œë”© ë…¼ë¬¸ Top {len(papers)}",
        "publication_date": episode_id,
        "created_at": datetime.now(timezone.utc).isoformat(),
        "papers": papers,
        "audio_file_path": f"https://storage.googleapis.com/{config.gcs_bucket_name}/episodes/{episode_id}/episode.mp3",
        "audio_duration": 1800,  # 30ë¶„
        "audio_size": 15000000,  # 15MB
        "status": "completed"
    }
    
    return episode_data


def upload_test_episode():
    """í…ŒìŠ¤íŠ¸ ì—í”¼ì†Œë“œë¥¼ GCSì— ì—…ë¡œë“œ"""
    print("ğŸš€ í…ŒìŠ¤íŠ¸ ì—í”¼ì†Œë“œ ë°ì´í„° ì—…ë¡œë“œ ì‹œì‘...")
    
    try:
        # GCS ì—…ë¡œë” ì´ˆê¸°í™”
        uploader = GCSUploader(
            bucket_name=config.gcs_bucket_name,
            credentials_path=config.google_credentials_path
        )
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        episode_data = create_test_episode_data()
        episode_id = episode_data["id"]
        
        print(f"ğŸ“„ ì—í”¼ì†Œë“œ ë°ì´í„° ìƒì„±: {episode_id}")
        
        # GCSì— ì—í”¼ì†Œë“œ ë©”íƒ€ë°ì´í„° ì—…ë¡œë“œ
        destination = f"podcasts/{episode_id}.json"
        public_url = uploader.upload_json(episode_data, destination)
        
        print(f"âœ… ì—í”¼ì†Œë“œ ë©”íƒ€ë°ì´í„° ì—…ë¡œë“œ ì„±ê³µ!")
        print(f"ğŸ”— Public URL: {public_url}")
        
        # ë”ë¯¸ ì˜¤ë””ì˜¤ íŒŒì¼ ìƒì„± ë° ì—…ë¡œë“œ
        print(f"ğŸµ ë”ë¯¸ ì˜¤ë””ì˜¤ íŒŒì¼ ìƒì„± ì¤‘...")
        import tempfile
        import os
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.mp3', delete=False) as temp_file:
            # ë”ë¯¸ MP3 íŒŒì¼ ë‚´ìš© (ì‹¤ì œë¡œëŠ” ë°”ì´ë„ˆë¦¬ ë°ì´í„°ì—¬ì•¼ í•¨)
            temp_file.write("ë”ë¯¸ ì˜¤ë””ì˜¤ íŒŒì¼ - ì‹¤ì œë¡œëŠ” MP3 ë°”ì´ë„ˆë¦¬ ë°ì´í„°")
            temp_audio_path = temp_file.name
        
        # ì˜¤ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ
        audio_destination = f"episodes/{episode_id}/episode.mp3"
        audio_url = uploader.upload_file(
            local_path=temp_audio_path,
            destination_path=audio_destination,
            content_type="audio/mpeg"
        )
        
        print(f"âœ… ì˜¤ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ ì„±ê³µ!")
        print(f"ğŸ”— Audio URL: {audio_url}")
        
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        os.unlink(temp_audio_path)
        
        # ìƒì„¸ ë©”íƒ€ë°ì´í„° ì—…ë¡œë“œ
        detailed_metadata = {
            "episode_id": episode_id,
            "title": episode_data["title"],
            "description": episode_data["description"],
            "papers": episode_data["papers"],
            "audio_url": audio_url,
            "created_at": episode_data["created_at"],
            "duration_seconds": episode_data["audio_duration"],
            "file_size_bytes": episode_data["audio_size"]
        }
        
        metadata_destination = f"episodes/{episode_id}/metadata.json"
        metadata_url = uploader.upload_json(detailed_metadata, metadata_destination)
        
        print(f"âœ… ìƒì„¸ ë©”íƒ€ë°ì´í„° ì—…ë¡œë“œ ì„±ê³µ!")
        print(f"ğŸ”— Metadata URL: {metadata_url}")
        
        print(f"\nğŸ‰ í…ŒìŠ¤íŠ¸ ì—í”¼ì†Œë“œ '{episode_id}' ì—…ë¡œë“œ ì™„ë£Œ!")
        print(f"ğŸ“¦ ë²„í‚·: {config.gcs_bucket_name}")
        print(f"ğŸ“ íŒŒì¼ë“¤:")
        print(f"  - podcasts/{episode_id}.json")
        print(f"  - episodes/{episode_id}/episode.mp3")
        print(f"  - episodes/{episode_id}/metadata.json")
        
        return episode_id
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì—í”¼ì†Œë“œ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ§ª GCS í…ŒìŠ¤íŠ¸ ì—í”¼ì†Œë“œ ë°ì´í„° ì—…ë¡œë“œ")
    print("=" * 50)
    
    # í™˜ê²½ ì„¤ì • í™•ì¸
    print(f"ğŸ”§ í™˜ê²½ ì„¤ì •:")
    print(f"  - GCS_BUCKET_NAME: {config.gcs_bucket_name}")
    print(f"  - GOOGLE_APPLICATION_CREDENTIALS: {config.google_credentials_path}")
    
    # í…ŒìŠ¤íŠ¸ ì—í”¼ì†Œë“œ ì—…ë¡œë“œ
    episode_id = upload_test_episode()
    
    if episode_id:
        print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print(f"ì´ì œ ë°±ì—”ë“œ APIì—ì„œ ì—í”¼ì†Œë“œ '{episode_id}'ë¥¼ ì¡°íšŒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print(f"í…ŒìŠ¤íŠ¸ URL: http://localhost:8001/api/episodes")
    else:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨!")
        return False
    
    return True


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
